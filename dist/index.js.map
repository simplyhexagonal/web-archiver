{
  "version": 3,
  "sources": ["../src/index.ts"],
  "sourcesContent": ["// @ts-ignore\nimport { version } from '../package.json';\n\nimport { resolve } from 'path';\nimport {\n  mkdirSync,\n  rmdirSync,\n  writeFileSync,\n} from 'fs';\nimport { URL } from 'url';\n\nimport MonoContext from '@simplyhexagonal/mono-context';\nimport Logger from '@simplyhexagonal/logger';\nimport { multiReplaceSync } from '@simplyhexagonal/multi-replace';\nimport puppeteer, {\n  Browser,\n  Page,\n} from 'puppeteer';\n\nexport interface WebArchiverRunOptions {\n  dryRun?: boolean;\n  urls: string[];\n  referer?: string;\n  outDir: string;\n  recursive: boolean;\n  maxRecursiveDepth: number;\n  allowNavigateAway?: boolean;\n  blackListUrls?: string[];\n}\n\nexport const UTF_RESOURCE_TYPES = [\n  'document',\n  'stylesheet',\n  'script',\n];\n\nclass WebArchiver {\n  static version = version;\n\n  logger: Logger;\n  browser?: Browser;\n  page?: Page;\n\n  constructor() {\n    const { webArchiverInstance, logger } = MonoContext.getState();\n\n    this.logger = logger || new Logger({});\n\n    if (webArchiverInstance) {\n      return webArchiverInstance;\n    }\n  }\n\n  async launchBrowser() {\n    const { logger } = this;\n\n    logger.debug('Launching browser...');\n\n    logger.time('Launched browser in');\n\n    this.browser = await puppeteer.launch({ headless: true });\n\n    const page = await this.browser.newPage();\n\n    await page.setRequestInterception(true);\n\n    this.page = page;\n\n    logger.timeEnd('Launched browser in');\n  }\n\n  async run({\n    dryRun,\n    urls,\n    referer,\n    outDir,\n    recursive,\n    maxRecursiveDepth,\n    allowNavigateAway,\n    blackListUrls,\n  }: WebArchiverRunOptions) {\n    const {\n      logger,\n      page,\n    } = this;\n\n    if (!dryRun) {\n      try {\n        mkdirSync(outDir, { recursive: true });\n      } catch (e: any) {\n        if (e.code !== 'EEXIST') {\n          await logger.error(e);\n\n          throw e;\n        } else {\n          logger.warn('Output directory already exists');\n        }\n      }\n    }\n\n    if (page) {\n      if (referer) {\n        page.setExtraHTTPHeaders({ referer });\n      }\n\n      const hosts = urls.map((url) => (new URL(url)).host);\n      const resourceTypeMap: any = {};\n      const filesToDownload: string[] = [];\n      const filesAlreadyDownloaded: string[] = [];\n      const processedUrls: string[] = [];\n      let currentRecursionDepth = 0;\n\n      await logger.info(`Running Web Archiver ${WebArchiver.version}`);\n\n      await logger.info(`Given urls:\\n\\n${urls.join('\\n')}`);\n\n      if (!dryRun) {\n        await logger.info(`Creating output directories...`);\n\n        await hosts.reduce(async (a, host) => {\n            await a;\n\n            try {\n              rmdirSync(resolve(outDir, host), { recursive: true });\n              mkdirSync(resolve(outDir, host), { recursive: true });\n            } catch (e: any) {\n              await logger.warn(e);\n            }\n\n            return Promise.resolve();\n          },\n          Promise.resolve(),\n        );\n\n        await logger.info(`Done creating output directories.`);\n      }\n\n      page.on('request', (interceptedRequest) => {\n        const u = interceptedRequest.url();\n        const url = new URL(u);\n\n        if (\n          !(blackListUrls || []).includes(u)\n          && (\n            allowNavigateAway || hosts.includes(url.host)\n          )\n        ) {\n          const resourceType = interceptedRequest.resourceType();\n\n          const outFilePath = resolve(\n            outDir,\n            url.host,\n            url.pathname.replace(/^\\//, ''),\n          );\n\n          if (!filesAlreadyDownloaded.includes(outFilePath)) {\n            filesToDownload.push(outFilePath);\n\n            resourceTypeMap[outFilePath] = resourceType;\n          }\n        }\n\n        interceptedRequest.continue();\n      });\n\n      page.on('response', async (interceptedResponse) => {\n        const url = new URL(interceptedResponse.url());\n\n        const {\n          'content-type': contentType,\n        } = interceptedResponse.headers();\n\n        let outFilePath = resolve(\n          outDir,\n          url.host,\n          url.pathname.replace(/^\\//, ''),\n        );\n\n        if (\n          filesToDownload.includes(outFilePath)\n          && !filesAlreadyDownloaded.includes(outFilePath)\n        ) {\n          filesAlreadyDownloaded.push(outFilePath);\n\n          if (!dryRun) {\n            const resourceType = resourceTypeMap[outFilePath];\n\n            if (resourceType === 'document' && contentType.includes('text/html')) {\n              outFilePath = multiReplaceSync(\n                outFilePath,\n                [\n                  [new RegExp(`\\/output\\/${url.host}$`), `/output/${url.host}/index.html`],\n                  [/\\/([^\\/\\.]*?)$/, '/$1/index.html'],\n                ],\n              );\n            }\n\n            await logger.debug(`Saving: ${outFilePath}`);\n\n            const encoding = (\n              UTF_RESOURCE_TYPES.includes(resourceTypeMap[outFilePath])\n            ) ? (\n              'utf8'\n            ) : (\n              'binary'\n            );\n\n            const outFileDir = outFilePath.replace(/\\/[^\\/]*?$/, '');\n\n            try {\n              mkdirSync(outFileDir, { recursive: true });\n            } catch (e: any) {\n              if (e.code !== 'EEXIST') {\n                await logger.error(e);\n\n                throw e;\n              }\n            }\n\n            const content = await interceptedResponse.buffer();\n\n            writeFileSync(outFilePath, content, { encoding });\n          }\n        }\n\n        if (!processedUrls.includes(`${url.host}${url.pathname}`)) {\n          processedUrls.push(`${url.host}${url.pathname}`);\n        }\n      });\n\n      let nextPages: string[] = [...urls];\n      let nextRound: string[] = [];\n      const processedPages: string[] = [];\n\n      while (nextPages.length > 0) {\n        const u = nextPages.shift() || '';\n\n        const url = new URL(u);\n\n        if (!processedUrls.includes(`${url.host}${url.pathname}`)) {\n          await logger.info(`Visiting: ${u}`);\n\n          processedUrls.push(`${url.host}${url.pathname}`);\n\n          const result = await page.goto(u, {\n            waitUntil: 'networkidle0',\n            referer,\n          });\n\n          processedPages.push(u);\n\n          if (result.status() === 200) {\n            const links = await page.$$('a');\n\n            (await Promise.all<string>(\n              (\n                await Promise.all(\n                  links.map((l) => l.getProperty('href'))\n                )\n              ).map((p) => p.jsonValue())\n            )).filter(\n              (l) => l && !(blackListUrls || []).includes(l)\n            ).map(\n              (l) => new URL(l)\n            ).filter(\n              (l) => {\n                return (\n                  !processedUrls.includes(`${l.host}${l.pathname}`)\n                  && (\n                    allowNavigateAway\n                    || hosts.includes(l.host)\n                  )\n                );\n              }\n            ).forEach(\n              (l) => {\n                nextRound.push(`${l.protocol}//${l.host}${l.pathname}`)\n              }\n            );\n          }\n        }\n\n        if (nextPages.length <= 1) {\n          currentRecursionDepth += 1;\n\n          if (recursive && currentRecursionDepth <= maxRecursiveDepth) {\n            nextPages = [...nextRound];\n            nextRound = [];\n          }\n        }\n      }\n\n      await logger.debug(\n        processedPages.length,\n        'urls:\\n\\n',\n        processedPages.join('\\n')\n      );\n\n      await logger.info(\n        filesAlreadyDownloaded.length,\n        'files:\\n\\n',\n        filesAlreadyDownloaded.map((f) => f.replace(outDir, '')).join('\\n')\n      );\n\n      await logger.info(`Done running Web Archiver ${WebArchiver.version}`);\n    }\n  }\n}\n\nexport default WebArchiver;\n"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;;;;;;AAGA,kBAAwB;AACxB,gBAIO;AACP,iBAAoB;AAEpB,0BAAwB;AACxB,oBAAmB;AACnB,2BAAiC;AACjC,uBAGO;AAaA,IAAM,qBAAqB;AAAA,EAChC;AAAA,EACA;AAAA,EACA;AAAA;AAGF,yBAAkB;AAAA,EAOhB,cAAc;AACZ,UAAM,EAAE,qBAAqB,WAAW,4BAAY;AAEpD,SAAK,SAAS,UAAU,IAAI,sBAAO;AAEnC,QAAI,qBAAqB;AACvB,aAAO;AAAA;AAAA;AAAA,QAIL,gBAAgB;AACpB,UAAM,EAAE,WAAW;AAEnB,WAAO,MAAM;AAEb,WAAO,KAAK;AAEZ,SAAK,UAAU,MAAM,yBAAU,OAAO,EAAE,UAAU;AAElD,UAAM,OAAO,MAAM,KAAK,QAAQ;AAEhC,UAAM,KAAK,uBAAuB;AAElC,SAAK,OAAO;AAEZ,WAAO,QAAQ;AAAA;AAAA,QAGX,IAAI;AAAA,IACR;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,KACwB;AACxB,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,QACE;AAEJ,QAAI,CAAC,QAAQ;AACX,UAAI;AACF,iCAAU,QAAQ,EAAE,WAAW;AAAA,eACxB,GAAP;AACA,YAAI,EAAE,SAAS,UAAU;AACvB,gBAAM,OAAO,MAAM;AAEnB,gBAAM;AAAA,eACD;AACL,iBAAO,KAAK;AAAA;AAAA;AAAA;AAKlB,QAAI,MAAM;AACR,UAAI,SAAS;AACX,aAAK,oBAAoB,EAAE;AAAA;AAG7B,YAAM,QAAQ,KAAK,IAAI,CAAC,QAAS,IAAI,eAAI,KAAM;AAC/C,YAAM,kBAAuB;AAC7B,YAAM,kBAA4B;AAClC,YAAM,yBAAmC;AACzC,YAAM,gBAA0B;AAChC,UAAI,wBAAwB;AAE5B,YAAM,OAAO,KAAK,wBAAwB,aAAY;AAEtD,YAAM,OAAO,KAAK;AAAA;AAAA,EAAkB,KAAK,KAAK;AAE9C,UAAI,CAAC,QAAQ;AACX,cAAM,OAAO,KAAK;AAElB,cAAM,MAAM,OAAO,OAAO,GAAG,SAAS;AAClC,gBAAM;AAEN,cAAI;AACF,qCAAU,yBAAQ,QAAQ,OAAO,EAAE,WAAW;AAC9C,qCAAU,yBAAQ,QAAQ,OAAO,EAAE,WAAW;AAAA,mBACvC,GAAP;AACA,kBAAM,OAAO,KAAK;AAAA;AAGpB,iBAAO,QAAQ;AAAA,WAEjB,QAAQ;AAGV,cAAM,OAAO,KAAK;AAAA;AAGpB,WAAK,GAAG,WAAW,CAAC,uBAAuB;AACzC,cAAM,IAAI,mBAAmB;AAC7B,cAAM,MAAM,IAAI,eAAI;AAEpB,YACE,CAAE,kBAAiB,IAAI,SAAS,MAE9B,sBAAqB,MAAM,SAAS,IAAI,QAE1C;AACA,gBAAM,eAAe,mBAAmB;AAExC,gBAAM,cAAc,yBAClB,QACA,IAAI,MACJ,IAAI,SAAS,QAAQ,OAAO;AAG9B,cAAI,CAAC,uBAAuB,SAAS,cAAc;AACjD,4BAAgB,KAAK;AAErB,4BAAgB,eAAe;AAAA;AAAA;AAInC,2BAAmB;AAAA;AAGrB,WAAK,GAAG,YAAY,OAAO,wBAAwB;AACjD,cAAM,MAAM,IAAI,eAAI,oBAAoB;AAExC,cAAM;AAAA,UACJ,gBAAgB;AAAA,YACd,oBAAoB;AAExB,YAAI,cAAc,yBAChB,QACA,IAAI,MACJ,IAAI,SAAS,QAAQ,OAAO;AAG9B,YACE,gBAAgB,SAAS,gBACtB,CAAC,uBAAuB,SAAS,cACpC;AACA,iCAAuB,KAAK;AAE5B,cAAI,CAAC,QAAQ;AACX,kBAAM,eAAe,gBAAgB;AAErC,gBAAI,iBAAiB,cAAc,YAAY,SAAS,cAAc;AACpE,4BAAc,2CACZ,aACA;AAAA,gBACE,CAAC,IAAI,OAAO,WAAa,IAAI,UAAU,WAAW,IAAI;AAAA,gBACtD,CAAC,kBAAkB;AAAA;AAAA;AAKzB,kBAAM,OAAO,MAAM,WAAW;AAE9B,kBAAM,WACJ,mBAAmB,SAAS,gBAAgB,gBAE5C,SAEA;AAGF,kBAAM,aAAa,YAAY,QAAQ,cAAc;AAErD,gBAAI;AACF,uCAAU,YAAY,EAAE,WAAW;AAAA,qBAC5B,GAAP;AACA,kBAAI,EAAE,SAAS,UAAU;AACvB,sBAAM,OAAO,MAAM;AAEnB,sBAAM;AAAA;AAAA;AAIV,kBAAM,UAAU,MAAM,oBAAoB;AAE1C,yCAAc,aAAa,SAAS,EAAE;AAAA;AAAA;AAI1C,YAAI,CAAC,cAAc,SAAS,GAAG,IAAI,OAAO,IAAI,aAAa;AACzD,wBAAc,KAAK,GAAG,IAAI,OAAO,IAAI;AAAA;AAAA;AAIzC,UAAI,YAAsB,CAAC,GAAG;AAC9B,UAAI,YAAsB;AAC1B,YAAM,iBAA2B;AAEjC,aAAO,UAAU,SAAS,GAAG;AAC3B,cAAM,IAAI,UAAU,WAAW;AAE/B,cAAM,MAAM,IAAI,eAAI;AAEpB,YAAI,CAAC,cAAc,SAAS,GAAG,IAAI,OAAO,IAAI,aAAa;AACzD,gBAAM,OAAO,KAAK,aAAa;AAE/B,wBAAc,KAAK,GAAG,IAAI,OAAO,IAAI;AAErC,gBAAM,SAAS,MAAM,KAAK,KAAK,GAAG;AAAA,YAChC,WAAW;AAAA,YACX;AAAA;AAGF,yBAAe,KAAK;AAEpB,cAAI,OAAO,aAAa,KAAK;AAC3B,kBAAM,QAAQ,MAAM,KAAK,GAAG;AAE5B,YAAC,OAAM,QAAQ,IAEX,OAAM,QAAQ,IACZ,MAAM,IAAI,CAAC,MAAM,EAAE,YAAY,WAEjC,IAAI,CAAC,MAAM,EAAE,eACd,OACD,CAAC,MAAM,KAAK,CAAE,kBAAiB,IAAI,SAAS,IAC5C,IACA,CAAC,MAAM,IAAI,eAAI,IACf,OACA,CAAC,MAAM;AACL,qBACE,CAAC,cAAc,SAAS,GAAG,EAAE,OAAO,EAAE,eAEpC,sBACG,MAAM,SAAS,EAAE;AAAA,eAI1B,QACA,CAAC,MAAM;AACL,wBAAU,KAAK,GAAG,EAAE,aAAa,EAAE,OAAO,EAAE;AAAA;AAAA;AAAA;AAMpD,YAAI,UAAU,UAAU,GAAG;AACzB,mCAAyB;AAEzB,cAAI,aAAa,yBAAyB,mBAAmB;AAC3D,wBAAY,CAAC,GAAG;AAChB,wBAAY;AAAA;AAAA;AAAA;AAKlB,YAAM,OAAO,MACX,eAAe,QACf,aACA,eAAe,KAAK;AAGtB,YAAM,OAAO,KACX,uBAAuB,QACvB,cACA,uBAAuB,IAAI,CAAC,MAAM,EAAE,QAAQ,QAAQ,KAAK,KAAK;AAGhE,YAAM,OAAO,KAAK,6BAA6B,aAAY;AAAA;AAAA;AAAA;AA5QjE;AACS,AADT,YACS,UAAU;AAgRnB,IAAO,cAAQ;",
  "names": []
}
